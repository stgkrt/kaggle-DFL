{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFL benchmark - training\n",
    "This is a simple benchmark script for DFL.  \n",
    "It classifies each frame image in the video into 4 classes（'background','challenge','play','throwin'） \n",
    "It does not use temporal information, so it may not be competitive on its own for this competition, but it could be used as a feature extractor for more advanced models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gc\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import timm\n",
    "from timm import utils\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR, CosineAnnealingLR, ReduceLROnPlateau, StepLR, LambdaLR\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import imageio\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    EXP = \"eff_b5_ap_bce_flowimage\"\n",
    "\n",
    "    # model\n",
    "    # model_path = \"swint_large224\" #effnetでやってみる？\n",
    "    model_path =  \"tf_efficientnet_b5_ap\"\n",
    "    MODEL_SAVE_DIR = f\"/workdir/work/output/{EXP}\"\n",
    "    out_features = 4 # output class\n",
    "    inp_channels = 3 #RGB -> 3\n",
    "    dropout = 0\n",
    "    pretrained = True\n",
    "\n",
    "    # train, valid\n",
    "    TRAIN_IMG_DIR = \"/workdir/work/output/train_images_flow\"\n",
    "    TRAIN_CSV = \"/workdir/work/output/saved_train_flowimages.csv\"\n",
    "    random_seed = 42\n",
    "    batch_size = 8\n",
    "    num_workers = 8\n",
    "    n_epoch = 100\n",
    "    early_stopping_rounds = 5\n",
    "    # n_fold = 5\n",
    "    # TRAIN_FOLD = [0, 1, 2, 3, 4]\n",
    "\n",
    "    img_height = 224\n",
    "    img_width = 224\n",
    "        \n",
    "    #optimizer\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000\n",
    "    opt_eps = 1e-5\n",
    "    # lr = 1e-6\n",
    "    lr = 1e-5\n",
    "    opt_wd_non_norm_bias = 0.01\n",
    "    opt_wd_norm_bias = 0\n",
    "\n",
    "    #scheduler\n",
    "    scheduler_name = \"CosineAnnealingWarmRestarts\"\n",
    "    # scheduler_name = \"OneCycleLR\"\n",
    "    T_0 = 5\n",
    "    # min_lr = 1e-7\n",
    "    # max_lr = 1e-5\n",
    "    min_lr = 1e-7\n",
    "    max_lr = 1e-4\n",
    "    T_max = 5\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    if DEBUG:\n",
    "        n_epoch = 1\n",
    "        TRAIN_FOLD = [0, 1]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "CFG.device = device\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model in this dir\n",
    "if not os.path.exists(CFG.MODEL_SAVE_DIR):\n",
    "    os.makedirs(CFG.MODEL_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger(log_file=f'{CFG.MODEL_SAVE_DIR}/train_{CFG.EXP}.log'):\n",
    "    \"\"\"Output Log.\"\"\"\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "LOGGER = init_logger()\n",
    "LOGGER.info(f\"EXP NAME = {CFG.EXP}\")\n",
    "LOGGER.info(f\"Model = {CFG.model_path}, (height, width) = ({CFG.img_height}, {CFG.img_width})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=0):\n",
    "    \"\"\"Fixed seed value.\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T13:46:56.464484Z",
     "iopub.status.busy": "2022-08-14T13:46:56.46364Z",
     "iopub.status.idle": "2022-08-14T13:46:56.562918Z",
     "shell.execute_reply": "2022-08-14T13:46:56.561751Z",
     "shell.execute_reply.started": "2022-08-14T13:46:56.464445Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(CFG.TRAIN_CSV)\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_encoding = {\n",
    "    \"background\" : 0,\n",
    "    \"challenge\" : 1,\n",
    "    \"play\" : 2,\n",
    "    \"throwin\" : 3,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oe = OneHotEncoder(categories=[[\"background\",\"challenge\",\"play\",\"throwin\"]], sparse=False)\n",
    "\n",
    "class DFLDataset(Dataset):\n",
    "    def __init__(self, video_id, frame, targets, transform=None):\n",
    "        self.video_id = video_id\n",
    "        self.frame = frame\n",
    "        self.targets = targets\n",
    "        # self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = f\"{CFG.TRAIN_IMG_DIR}/{self.video_id[idx]}_{self.frame[idx]:06}.jpg\"\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, dsize=(CFG.img_height, CFG.img_width))\n",
    "        image = image / 255 # convert to 0-1\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "        # if self.transform is not None:\n",
    "        #     image = self.transform(image = image)[\"image\"]\n",
    "        target_idx = event_encoding[self.targets[idx]]\n",
    "        target = np.zeros(CFG.out_features).astype(np.float32)\n",
    "        target[target_idx] = 1\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFLNet(nn.Module):\n",
    "    def __init__(self, model_name=CFG.model_path, \n",
    "                 out_features=CFG.out_features, inp_channels=CFG.inp_channels,\n",
    "                 pretrained=CFG.pretrained):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels, num_classes=out_features)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        output = self.model(image)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha= 0.25, gamma=2.0, eps=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = self.alpha * ((1 - p) ** self.gamma) * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "            'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer):\n",
    "    scheduler = None\n",
    "    if CFG.scheduler_name == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0 = CFG.T_0,\n",
    "            eta_min = CFG.min_lr,\n",
    "            last_epoch=-1\n",
    "        )\n",
    "    elif CFG.scheduler_name == 'OneCycleLR':\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr = CFG.max_lr,\n",
    "            steps_per_epoch = int( ( (CFG.n_fold-1) * train_df.shape[0]) / (CFG.n_fold * CFG.batch_size) ) + 1,\n",
    "            epochs = CFG.n_epoch,\n",
    "        )\n",
    "\n",
    "    elif CFG.scheduler_name == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max = CFG.T_max,\n",
    "            eta_min = CFG.min_lr,\n",
    "            last_epoch = -1\n",
    "        )\n",
    "    \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divice_norm_bias(model): \n",
    "    norm_bias_params = []\n",
    "    non_norm_bias_params = []\n",
    "    except_wd_layers = ['norm', '.bias']\n",
    "    for n, p in model.model.named_parameters():\n",
    "        if any([nd in n for nd in except_wd_layers]):\n",
    "            norm_bias_params.append(p)\n",
    "        else:\n",
    "            non_norm_bias_params.append(p)\n",
    "    return norm_bias_params, non_norm_bias_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler=None, scaler=None):\n",
    "    model.train()\n",
    "    # stream = tqdm(train_loader)\n",
    "    losses = AverageMeter()\n",
    "    global_step = 0\n",
    "\n",
    "    # for step, (images, targets) in enumerate(stream, start=1):\n",
    "    for step, (images, targets) in enumerate(train_loader):\n",
    "        images = images.to(CFG.device, non_blocking=True)\n",
    "        targets = targets.to(CFG.device, non_blocking=True)\n",
    "\n",
    "        preds = model(images)\n",
    "        preds_softmax = softmax(preds)\n",
    "\n",
    "        loss = criterion(preds_softmax, targets)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        losses.update(loss.item(), CFG.batch_size) \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(val_loader, model, criterion, epoch):\n",
    "    model.eval()\n",
    "    # stream = tqdm(val_loader)\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    top1_m = AverageMeter()\n",
    "    top5_m = AverageMeter()\n",
    "    \n",
    "    final_targets = []\n",
    "    final_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # for i, (images, targets) in enumerate(stream, start=1):\n",
    "        for i, (images, targets) in enumerate(valid_loader, start=1):\n",
    "            images = images.to(CFG.device, non_blocking=True)\n",
    "            targets = targets.to(CFG.device, non_blocking=True)\n",
    "            preds = model(images)\n",
    "            preds_softmax = softmax(preds)\n",
    "\n",
    "            loss = criterion(preds_softmax, targets)\n",
    "            losses.update(loss.item(), CFG.batch_size)\n",
    "\n",
    "            targets_list = (targets.detach().cpu().numpy()).tolist()\n",
    "            preds_list = torch.argmax(preds, dim=1).tolist()\n",
    "            \n",
    "            final_targets.extend(targets_list)\n",
    "            final_preds.extend(preds_list)\n",
    "    return losses.avg, final_preds, final_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_videos ['1606b0e6_0' '1606b0e6_1']\n",
      "valid_videos ['cfbe2e94_0' 'cfbe2e94_1']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1606b0e6_0' '1606b0e6_1' 'cfbe2e94_0' 'cfbe2e94_1']\n"
     ]
    }
   ],
   "source": [
    "train_valid_videos = train_df[\"video_id\"].unique()\n",
    "print(train_valid_videos)\n",
    "train_videos = train_valid_videos[:2]\n",
    "valid_videos = train_valid_videos[2:]\n",
    "if DEBUG:\n",
    "    train_videos = [train_videos[0]]\n",
    "    valid_videos = [valid_videos[0]]\n",
    "LOGGER.info(f\"train_videos {train_videos}\")\n",
    "LOGGER.info(f\"valid_videos {valid_videos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/workdir/work/input/train/cfbe2e94_0.mp4', '/workdir/work/input/train/cfbe2e94_1.mp4']\n"
     ]
    }
   ],
   "source": [
    "# use for scoring\n",
    "valid_video_files = []\n",
    "for valid_video in valid_videos:\n",
    "    valid_video_files.append(f'/workdir/work/input/train/{valid_video}.mp4')\n",
    "print(valid_video_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate train/valid data \n",
    "X_train_videoid = train_df[train_df[\"video_id\"].isin(train_videos)][\"video_id\"].values\n",
    "X_train_frame = train_df[train_df[\"video_id\"].isin(train_videos)][\"frame\"].values\n",
    "y_train = train_df[train_df[\"video_id\"].isin(train_videos)][\"event\"].values\n",
    "\n",
    "X_valid_videoid = train_df[train_df[\"video_id\"].isin(valid_videos)][\"video_id\"].values\n",
    "X_valid_frame = train_df[train_df[\"video_id\"].isin(valid_videos)][\"frame\"].values\n",
    "y_valid = train_df[train_df[\"video_id\"].isin(valid_videos)][\"event\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "train_dataset = DFLDataset(video_id= X_train_videoid, frame=X_train_frame, targets = y_train)\n",
    "valid_dataset = DFLDataset(video_id= X_valid_videoid, frame=X_valid_frame, targets = y_valid)\n",
    "\n",
    "# create dataloader\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                        batch_size = CFG.batch_size,\n",
    "                        shuffle = False,\n",
    "                        num_workers = CFG.num_workers)\n",
    "valid_loader = DataLoader(valid_dataset,\n",
    "                        batch_size = CFG.batch_size,\n",
    "                        shuffle = False,\n",
    "                        num_workers = CFG.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model, cost function and optimizer\n",
    "model = DFLNet()\n",
    "model = model.to(device)\n",
    "\n",
    "norm_bias_params, non_norm_bias_params = divice_norm_bias(model)\n",
    "# criterion = FocalLoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "#print(f\"norm bias params: {len(norm_bias_params)}, non norm bias params: {len(non_norm_bias_params)}\")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {'params': norm_bias_params, 'weight_decay': CFG.opt_wd_norm_bias},\n",
    "        {'params': non_norm_bias_params, 'weight_decay': CFG.opt_wd_non_norm_bias},\n",
    "    ],\n",
    "    eps = CFG.opt_eps,\n",
    "    lr = CFG.lr,\n",
    "    amsgrad = False\n",
    ")\n",
    "\n",
    "# load scaler\n",
    "scheduler = get_scheduler(optimizer)\n",
    "scaler = GradScaler()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss 1.021262,  Valid loss 1.112889.\n",
      "Accuracy 0.223562. elapsed time:0.3 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 2: Train loss 0.462280,  Valid loss 1.046988.\n",
      "Accuracy 0.246753. elapsed time:0.6 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 3: Train loss 0.220568,  Valid loss 1.001004.\n",
      "Accuracy 0.265306. elapsed time:0.8 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 4: Train loss 0.100609,  Valid loss 0.982146.\n",
      "Accuracy 0.282004. elapsed time:1.1 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 5: Train loss 0.050220,  Valid loss 0.967092.\n",
      "Accuracy 0.297774. elapsed time:1.4 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 6: Train loss 0.030510,  Valid loss 0.958531.\n",
      "Accuracy 0.313544. elapsed time:1.6 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 7: Train loss 0.021602,  Valid loss 0.952879.\n",
      "Accuracy 0.316327. elapsed time:1.9 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 8: Train loss 0.016407,  Valid loss 0.948620.\n",
      "Accuracy 0.320965. elapsed time:2.2 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 9: Train loss 0.013086,  Valid loss 0.945353.\n",
      "Accuracy 0.324675. elapsed time:2.5 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 10: Train loss 0.010738,  Valid loss 0.942781.\n",
      "Accuracy 0.329314. elapsed time:2.7 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 11: Train loss 0.009007,  Valid loss 0.940603.\n",
      "Accuracy 0.334879. elapsed time:3.0 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 12: Train loss 0.007675,  Valid loss 0.938897.\n",
      "Accuracy 0.339518. elapsed time:3.3 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 13: Train loss 0.006624,  Valid loss 0.937497.\n",
      "Accuracy 0.339518. elapsed time:3.6 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 14: Train loss 0.005781,  Valid loss 0.936506.\n",
      "Accuracy 0.343228. elapsed time:3.8 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 15: Train loss 0.005089,  Valid loss 0.935653.\n",
      "Accuracy 0.346011. elapsed time:4.1 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 16: Train loss 0.004514,  Valid loss 0.934926.\n",
      "Accuracy 0.348794. elapsed time:4.4 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 17: Train loss 0.004031,  Valid loss 0.934333.\n",
      "Accuracy 0.350649. elapsed time:4.6 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 18: Train loss 0.003618,  Valid loss 0.933928.\n",
      "Accuracy 0.352505. elapsed time:4.9 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 19: Train loss 0.003265,  Valid loss 0.933682.\n",
      "Accuracy 0.356215. elapsed time:5.2 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 20: Train loss 0.002958,  Valid loss 0.933481.\n",
      "Accuracy 0.358071. elapsed time:5.5 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 21: Train loss 0.002692,  Valid loss 0.933358.\n",
      "Accuracy 0.360853. elapsed time:5.7 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 22: Train loss 0.002457,  Valid loss 0.933270.\n",
      "Accuracy 0.361781. elapsed time:6.0 min.\n",
      "Model is improved.\n",
      "/workdir/work/output/eff_b5_ap_bce_flowimage/tf_efficientnet_b5_ap.pth is saved.\n",
      "Epoch 23: Train loss 0.002249,  Valid loss 0.933272.\n",
      "Accuracy 0.361781. elapsed time:6.3 min.\n",
      "Epoch 24: Train loss 0.002066,  Valid loss 0.933401.\n",
      "Accuracy 0.363636. elapsed time:6.6 min.\n",
      "Epoch 25: Train loss 0.001902,  Valid loss 0.933595.\n",
      "Accuracy 0.364564. elapsed time:6.8 min.\n",
      "Epoch 26: Train loss 0.001755,  Valid loss 0.933712.\n",
      "Accuracy 0.364564. elapsed time:7.1 min.\n",
      "Epoch 27: Train loss 0.001623,  Valid loss 0.933915.\n",
      "Accuracy 0.366419. elapsed time:7.4 min.\n",
      "Early stopping. Model is not improved in 5 epochs\n",
      "Learning finished.\n"
     ]
    }
   ],
   "source": [
    "# train / valid loop\n",
    "# best_score = -9999.\n",
    "best_loss = 1e10\n",
    "ealry_stopping_count = 0\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1, CFG.n_epoch + 1):\n",
    "    train_avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, scaler)\n",
    "    valid_avg_loss, preds, targets = valid_fn(valid_loader, model, criterion, epoch)\n",
    "    accuracy = accuracy_score(np.argmax(targets, axis=1), preds)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    elapsed_min = elapsed/60\n",
    "    LOGGER.info(f\"Epoch {epoch}: Train loss {train_avg_loss:.6f},  Valid loss {valid_avg_loss:.6f}.\")\n",
    "    LOGGER.info(f\"Accuracy {accuracy:4f}. elapsed time:{elapsed_min:.1f} min.\")\n",
    "    if valid_avg_loss < best_loss:\n",
    "        LOGGER.info(f\"Model is improved.\")\n",
    "        ealry_stopping_count = 0\n",
    "        best_loss = valid_avg_loss\n",
    "        model_name = CFG.model_path\n",
    "        LOGGER.info(f'{CFG.MODEL_SAVE_DIR}/{model_name}.pth is saved.')\n",
    "        torch.save(model.state_dict(), f'{CFG.MODEL_SAVE_DIR}/{model_name}.pth')\n",
    "\n",
    "    else:\n",
    "        ealry_stopping_count += 1\n",
    "        if ealry_stopping_count >= CFG.early_stopping_rounds:\n",
    "            LOGGER.info(f\"Early stopping. Model is not improved in {CFG.early_stopping_rounds} epochs\")\n",
    "            break\n",
    "del model, train_loader, train_dataset\n",
    "gc.collect()\n",
    "\n",
    "LOGGER.info(\"Learning finished.\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate valid score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoringDataset(Dataset):\n",
    "    def __init__(self, video_id, frame, targets, transform=None):\n",
    "        self.video_id = video_id\n",
    "        self.frame = frame\n",
    "        self.targets = targets\n",
    "        # self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_id = self.video_id[idx]\n",
    "        frame = self.frame[idx]\n",
    "        image_path = f\"{CFG.TRAIN_IMG_DIR}/{self.video_id[idx]}_{self.frame[idx]:06}.jpg\"\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, dsize=(CFG.img_height, CFG.img_width))\n",
    "        image = image / 255 # convert to 0-1\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "        # if self.transform is not None:\n",
    "        #     image = self.transform(image = image)[\"image\"]\n",
    "        target_idx = event_encoding[self.targets[idx]]\n",
    "        target = np.zeros(CFG.out_features).astype(np.float32)\n",
    "        target[target_idx] = 1\n",
    "\n",
    "        return image, target, frame, video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "scoring_dataset = ScoringDataset(video_id= X_valid_videoid, frame=X_valid_frame, targets = y_valid)\n",
    "\n",
    "# create dataloader\n",
    "scoring_loader = DataLoader(scoring_dataset,\n",
    "                        batch_size = CFG.batch_size,\n",
    "                        shuffle = False,\n",
    "                        num_workers = CFG.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_decoding = {\n",
    "    0 : \"background\",\n",
    "    1 : \"challenge\",\n",
    "    2 : \"play\",\n",
    "    3 : \"throwin\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DFLNet(\n",
       "  (model): EfficientNet(\n",
       "    (conv_stem): Conv2dSame(3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): SiLU(inplace=True)\n",
       "    (blocks): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (bn1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): Identity()\n",
       "        )\n",
       "        (1): DepthwiseSeparableConv(\n",
       "          (conv_dw): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pw): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): Identity()\n",
       "        )\n",
       "        (2): DepthwiseSeparableConv(\n",
       "          (conv_dw): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pw): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(144, 144, kernel_size=(3, 3), stride=(2, 2), groups=144, bias=False)\n",
       "          (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
       "          (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
       "          (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
       "          (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
       "          (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(240, 240, kernel_size=(5, 5), stride=(2, 2), groups=240, bias=False)\n",
       "          (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "          (bn2): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "          (bn2): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "          (bn2): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "          (bn2): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(384, 384, kernel_size=(3, 3), stride=(2, 2), groups=384, bias=False)\n",
       "          (bn2): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "          (bn2): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "          (bn2): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "          (bn2): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "          (bn2): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): InvertedResidual(\n",
       "          (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "          (bn2): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (6): InvertedResidual(\n",
       "          (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "          (bn2): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
       "          (bn2): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(768, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (6): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(176, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(1056, 1056, kernel_size=(5, 5), stride=(2, 2), groups=1056, bias=False)\n",
       "          (bn2): BatchNorm2d(1056, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1056, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (6): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (7): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (8): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "          (bn2): BatchNorm2d(1824, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1824, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
       "          (bn2): BatchNorm2d(3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
       "          (bn2): BatchNorm2d(3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_head): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): SiLU(inplace=True)\n",
       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "    (classifier): Linear(in_features=2048, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DFLNet()\n",
    "model.load_state_dict(torch.load(f'{CFG.MODEL_SAVE_DIR}/{model_name}.pth'))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_subformat_list = []\n",
    "all_pred_logits = []\n",
    "\n",
    "for i, (images, targets, frames, video_id) in enumerate(scoring_loader):\n",
    "    images = images.to(CFG.device, non_blocking=True)\n",
    "    targets = targets.to(CFG.device, non_blocking=True)\n",
    "    frames = frames.to('cpu').detach().numpy().copy()\n",
    "\n",
    "    output = model(images)\n",
    "    output = softmax(output)\n",
    "    output = output.to('cpu').detach().numpy().copy()\n",
    "    targets_list = (targets.detach().cpu().numpy()).tolist()\n",
    "\n",
    "    preds_logits = output\n",
    "    all_pred_logits.extend(list(preds_logits))\n",
    "    preds_argmax_idx = np.argmax(preds_logits, axis=1)\n",
    "    preds_prob = [pred_logits[idx] for idx, pred_logits in zip(preds_argmax_idx, preds_logits)]\n",
    "    preds_event = [event_decoding[idx] for idx in preds_argmax_idx]\n",
    "    for idx, pred_argmax_idx in enumerate(preds_argmax_idx):\n",
    "        if pred_argmax_idx != 0:\n",
    "            pred_subformat_list.append([video_id[idx], frames[idx]/25, preds_event[idx], preds_prob[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>time</th>\n",
       "      <th>event</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>229.32</td>\n",
       "      <td>play</td>\n",
       "      <td>0.945952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>230.20</td>\n",
       "      <td>play</td>\n",
       "      <td>0.990414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>232.52</td>\n",
       "      <td>challenge</td>\n",
       "      <td>0.611308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>246.64</td>\n",
       "      <td>play</td>\n",
       "      <td>0.939598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>247.84</td>\n",
       "      <td>play</td>\n",
       "      <td>0.892802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>249.92</td>\n",
       "      <td>play</td>\n",
       "      <td>0.607771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>251.68</td>\n",
       "      <td>play</td>\n",
       "      <td>0.647425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>252.72</td>\n",
       "      <td>play</td>\n",
       "      <td>0.415689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>254.88</td>\n",
       "      <td>play</td>\n",
       "      <td>0.978792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>256.24</td>\n",
       "      <td>play</td>\n",
       "      <td>0.781794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     video_id    time      event     score\n",
       "0  cfbe2e94_0  229.32       play  0.945952\n",
       "1  cfbe2e94_0  230.20       play  0.990414\n",
       "2  cfbe2e94_0  232.52  challenge  0.611308\n",
       "3  cfbe2e94_0  246.64       play  0.939598\n",
       "4  cfbe2e94_0  247.84       play  0.892802\n",
       "5  cfbe2e94_0  249.92       play  0.607771\n",
       "6  cfbe2e94_0  251.68       play  0.647425\n",
       "7  cfbe2e94_0  252.72       play  0.415689\n",
       "8  cfbe2e94_0  254.88       play  0.978792\n",
       "9  cfbe2e94_0  256.24       play  0.781794"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>time</th>\n",
       "      <th>event</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>3510.84</td>\n",
       "      <td>play</td>\n",
       "      <td>0.524167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>3511.36</td>\n",
       "      <td>play</td>\n",
       "      <td>0.838474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>3518.56</td>\n",
       "      <td>play</td>\n",
       "      <td>0.485214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>3520.40</td>\n",
       "      <td>play</td>\n",
       "      <td>0.965838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>3523.24</td>\n",
       "      <td>play</td>\n",
       "      <td>0.894332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>3524.40</td>\n",
       "      <td>play</td>\n",
       "      <td>0.897755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>3561.32</td>\n",
       "      <td>play</td>\n",
       "      <td>0.541905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>3562.64</td>\n",
       "      <td>play</td>\n",
       "      <td>0.634178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>3572.48</td>\n",
       "      <td>play</td>\n",
       "      <td>0.343528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>3574.32</td>\n",
       "      <td>play</td>\n",
       "      <td>0.418061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       video_id     time event     score\n",
       "925  cfbe2e94_1  3510.84  play  0.524167\n",
       "926  cfbe2e94_1  3511.36  play  0.838474\n",
       "927  cfbe2e94_1  3518.56  play  0.485214\n",
       "928  cfbe2e94_1  3520.40  play  0.965838\n",
       "929  cfbe2e94_1  3523.24  play  0.894332\n",
       "930  cfbe2e94_1  3524.40  play  0.897755\n",
       "931  cfbe2e94_1  3561.32  play  0.541905\n",
       "932  cfbe2e94_1  3562.64  play  0.634178\n",
       "933  cfbe2e94_1  3572.48  play  0.343528\n",
       "934  cfbe2e94_1  3574.32  play  0.418061"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "play         656\n",
       "challenge    154\n",
       "throwin      125\n",
       "Name: event, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_df = pd.DataFrame(pred_subformat_list, columns=[\"video_id\", \"time\", \"event\", \"score\"])\n",
    "display(scoring_df.head(10))\n",
    "display(scoring_df.tail(10))\n",
    "scoring_df[\"event\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_df.to_csv(f\"/workdir/work/output/{CFG.EXP}/validation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from https://www.kaggle.com/code/ryanholbrook/competition-metric-dfl-event-detection-ap\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.testing import assert_index_equal\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "tolerances = {\n",
    "    \"challenge\": [0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "    \"play\": [0.15, 0.20, 0.25, 0.30, 0.35],\n",
    "    \"throwin\": [0.15, 0.20, 0.25, 0.30, 0.35],\n",
    "}\n",
    "\n",
    "def filter_detections(\n",
    "        detections: pd.DataFrame, intervals: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Drop detections not inside a scoring interval.\"\"\"\n",
    "    detection_time = detections.loc[:, 'time'].sort_values().to_numpy()\n",
    "    intervals = intervals.to_numpy()\n",
    "    is_scored = np.full_like(detection_time, False, dtype=bool)\n",
    "\n",
    "    i, j = 0, 0\n",
    "    while i < len(detection_time) and j < len(intervals):\n",
    "        time = detection_time[i]\n",
    "        int_ = intervals[j]\n",
    "\n",
    "        # If the detection is prior in time to the interval, go to the next detection.\n",
    "        if time < int_.left:\n",
    "            i += 1\n",
    "        # If the detection is inside the interval, keep it and go to the next detection.        \n",
    "        elif time in int_:\n",
    "            is_scored[i] = True\n",
    "            i += 1\n",
    "        # If the detection is later in time, go to the next interval.\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "    return detections.loc[is_scored].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def match_detections(\n",
    "        tolerance: float, ground_truths: pd.DataFrame, detections: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Match detections to ground truth events. Arguments are taken from a common event x tolerance x video evaluation group.\"\"\"\n",
    "    detections_sorted = detections.sort_values('score', ascending=False).dropna()\n",
    "\n",
    "    is_matched = np.full_like(detections_sorted['event'], False, dtype=bool)\n",
    "    gts_matched = set()\n",
    "    for i, det in enumerate(detections_sorted.itertuples(index=False)):\n",
    "        best_error = tolerance\n",
    "        best_gt = None\n",
    "\n",
    "        for gt in ground_truths.itertuples(index=False):\n",
    "            error = abs(det.time - gt.time)\n",
    "            if error < best_error and not gt in gts_matched:\n",
    "                best_gt = gt\n",
    "                best_error = error\n",
    "            \n",
    "        if best_gt is not None:\n",
    "            is_matched[i] = True\n",
    "            gts_matched.add(best_gt)\n",
    "\n",
    "    detections_sorted['matched'] = is_matched\n",
    "\n",
    "    return detections_sorted\n",
    "\n",
    "\n",
    "def precision_recall_curve(\n",
    "        matches: np.ndarray, scores: np.ndarray, p: int\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    if len(matches) == 0:\n",
    "        return [1], [0], []\n",
    "\n",
    "    # Sort matches by decreasing confidence\n",
    "    idxs = np.argsort(scores, kind='stable')[::-1]\n",
    "    scores = scores[idxs]\n",
    "    matches = matches[idxs]\n",
    "    \n",
    "    distinct_value_indices = np.where(np.diff(scores))[0]\n",
    "    threshold_idxs = np.r_[distinct_value_indices, matches.size - 1]\n",
    "    thresholds = scores[threshold_idxs]\n",
    "    \n",
    "    # Matches become TPs and non-matches FPs as confidence threshold decreases\n",
    "    tps = np.cumsum(matches)[threshold_idxs]\n",
    "    fps = np.cumsum(~matches)[threshold_idxs]\n",
    "    \n",
    "    precision = tps / (tps + fps)\n",
    "    precision[np.isnan(precision)] = 0\n",
    "    recall = tps / p  # total number of ground truths might be different than total number of matches\n",
    "    \n",
    "    # Stop when full recall attained and reverse the outputs so recall is non-increasing.\n",
    "    last_ind = tps.searchsorted(tps[-1])\n",
    "    sl = slice(last_ind, None, -1)\n",
    "\n",
    "    # Final precision is 1 and final recall is 0\n",
    "    return np.r_[precision[sl], 1], np.r_[recall[sl], 0], thresholds[sl]\n",
    "\n",
    "\n",
    "def average_precision_score(matches: np.ndarray, scores: np.ndarray, p: int) -> float:\n",
    "    precision, recall, _ = precision_recall_curve(matches, scores, p)\n",
    "    # Compute step integral\n",
    "    return -np.sum(np.diff(recall) * np.array(precision)[:-1])\n",
    "\n",
    "\n",
    "def event_detection_ap(\n",
    "        solution: pd.DataFrame,\n",
    "        submission: pd.DataFrame,\n",
    "        tolerances: Dict[str, float],\n",
    ") -> float:\n",
    "\n",
    "    assert_index_equal(solution.columns, pd.Index(['video_id', 'time', 'event']))\n",
    "    assert_index_equal(submission.columns, pd.Index(['video_id', 'time', 'event', 'score']))\n",
    "\n",
    "    # Ensure solution and submission are sorted properly\n",
    "    solution = solution.sort_values(['video_id', 'time'])\n",
    "    submission = submission.sort_values(['video_id', 'time'])\n",
    "    \n",
    "    # Extract scoring intervals.\n",
    "    intervals = (\n",
    "        solution\n",
    "        .query(\"event in ['start', 'end']\")\n",
    "        .assign(interval=lambda x: x.groupby(['video_id', 'event']).cumcount())\n",
    "        .pivot(index='interval', columns=['video_id', 'event'], values='time')\n",
    "        .stack('video_id')\n",
    "        .swaplevel()\n",
    "        .sort_index()\n",
    "        .loc[:, ['start', 'end']]\n",
    "        .apply(lambda x: pd.Interval(*x, closed='both'), axis=1)\n",
    "    )\n",
    "\n",
    "    # Extract ground-truth events.\n",
    "    ground_truths = (\n",
    "        solution\n",
    "        .query(\"event not in ['start', 'end']\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Map each event class to its prevalence (needed for recall calculation)\n",
    "    class_counts = ground_truths.value_counts('event').to_dict()\n",
    "\n",
    "    # Create table for detections with a column indicating a match to a ground-truth event\n",
    "    detections = submission.assign(matched = False)\n",
    "\n",
    "    # Remove detections outside of scoring intervals\n",
    "    detections_filtered = []\n",
    "    for (det_group, dets), (int_group, ints) in zip(\n",
    "        detections.groupby('video_id'), intervals.groupby('video_id')\n",
    "    ):\n",
    "        assert det_group == int_group\n",
    "        detections_filtered.append(filter_detections(dets, ints))\n",
    "    detections_filtered = pd.concat(detections_filtered, ignore_index=True)\n",
    "\n",
    "    # Create table of event-class x tolerance x video_id values\n",
    "    aggregation_keys = pd.DataFrame(\n",
    "        [(ev, tol, vid)\n",
    "         for ev in tolerances.keys()\n",
    "         for tol in tolerances[ev]\n",
    "         for vid in ground_truths['video_id'].unique()],\n",
    "        columns=['event', 'tolerance', 'video_id'],\n",
    "    )\n",
    "\n",
    "    # Create match evaluation groups: event-class x tolerance x video_id\n",
    "    detections_grouped = (\n",
    "        aggregation_keys\n",
    "        .merge(detections_filtered, on=['event', 'video_id'], how='left')\n",
    "        .groupby(['event', 'tolerance', 'video_id'])\n",
    "    )\n",
    "    ground_truths_grouped = (\n",
    "        aggregation_keys\n",
    "        .merge(ground_truths, on=['event', 'video_id'], how='left')\n",
    "        .groupby(['event', 'tolerance', 'video_id'])\n",
    "    )\n",
    "    \n",
    "    # Match detections to ground truth events by evaluation group\n",
    "    detections_matched = []\n",
    "    for key in aggregation_keys.itertuples(index=False):\n",
    "        dets = detections_grouped.get_group(key)\n",
    "        gts = ground_truths_grouped.get_group(key)\n",
    "        detections_matched.append(\n",
    "            match_detections(dets['tolerance'].iloc[0], gts, dets)\n",
    "        )\n",
    "    detections_matched = pd.concat(detections_matched)\n",
    "    \n",
    "    # Compute AP per event x tolerance group\n",
    "    event_classes = ground_truths['event'].unique()\n",
    "    ap_table = (\n",
    "        detections_matched\n",
    "        .query(\"event in @event_classes\")\n",
    "        .groupby(['event', 'tolerance']).apply(\n",
    "        lambda group: average_precision_score(\n",
    "        group['matched'].to_numpy(),\n",
    "                group['score'].to_numpy(),\n",
    "                class_counts[group['event'].iat[0]],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Average over tolerances, then over event classes\n",
    "    mean_ap = ap_table.groupby('event').mean().mean()\n",
    "\n",
    "    return mean_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>time</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1606b0e6_0</td>\n",
       "      <td>200.265822</td>\n",
       "      <td>start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1606b0e6_0</td>\n",
       "      <td>201.150000</td>\n",
       "      <td>challenge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1606b0e6_0</td>\n",
       "      <td>202.765822</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1606b0e6_0</td>\n",
       "      <td>210.124111</td>\n",
       "      <td>start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1606b0e6_0</td>\n",
       "      <td>210.870000</td>\n",
       "      <td>challenge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     video_id        time      event\n",
       "0  1606b0e6_0  200.265822      start\n",
       "1  1606b0e6_0  201.150000  challenge\n",
       "2  1606b0e6_0  202.765822        end\n",
       "3  1606b0e6_0  210.124111      start\n",
       "4  1606b0e6_0  210.870000  challenge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solution = pd.read_csv(\"/workdir/work/input/train.csv\", usecols=['video_id', 'time', 'event'])\n",
    "display(solution.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>time</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8652</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>229.321518</td>\n",
       "      <td>start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8653</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>230.200000</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8654</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>232.520000</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8655</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>234.016200</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8656</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>246.666301</td>\n",
       "      <td>start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10233</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>3562.660000</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10234</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>3563.835896</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10235</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>3572.500727</td>\n",
       "      <td>start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10236</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>3574.340000</td>\n",
       "      <td>throwin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10237</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>3575.000727</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1586 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id         time    event\n",
       "8652   cfbe2e94_0   229.321518    start\n",
       "8653   cfbe2e94_0   230.200000     play\n",
       "8654   cfbe2e94_0   232.520000     play\n",
       "8655   cfbe2e94_0   234.016200      end\n",
       "8656   cfbe2e94_0   246.666301    start\n",
       "...           ...          ...      ...\n",
       "10233  cfbe2e94_1  3562.660000     play\n",
       "10234  cfbe2e94_1  3563.835896      end\n",
       "10235  cfbe2e94_1  3572.500727    start\n",
       "10236  cfbe2e94_1  3574.340000  throwin\n",
       "10237  cfbe2e94_1  3575.000727      end\n",
       "\n",
       "[1586 rows x 3 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution[solution['video_id'].isin(valid_videos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1882092797717849\n"
     ]
    }
   ],
   "source": [
    "score_just_pred= event_detection_ap(solution[solution['video_id'].isin(valid_videos)], scoring_df, tolerances)\n",
    "print(score_just_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scoring with post proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_names = ['challenge', 'throwin', 'play']\n",
    "label_dict = {\n",
    "    'background':0,\n",
    "    'challenge':1,\n",
    "    'play':2,\n",
    "    'throwin':3,\n",
    "}\n",
    "event_names_with_background = ['background','challenge','play','throwin']\n",
    "\n",
    "def make_sub(prob, pred_df):\n",
    "    \n",
    "    frame_rate = 25\n",
    "    window_size = 10\n",
    "    ignore_width = 10\n",
    "    group_count = 5\n",
    "\n",
    "    df = pd.DataFrame(prob,columns=event_names_with_background)\n",
    "    df['video_id'] = pred_df['video_id']\n",
    "    df['frame_id'] = pred_df['time']*frame_rate\n",
    "\n",
    "    train_df = pd.DataFrame()\n",
    "    for video_id, each_video_df in df.groupby('video_id'):\n",
    "        for i, event in enumerate(event_names):\n",
    "            # イベント毎にwindow size分の移動平均を取る-> prob_arrに格納(最初と最後のwindow_sizeがたりない分はNanになるので-100で埋める)\n",
    "            prob_arr = each_video_df[event].rolling(window=window_size, center=True).mean().fillna(-100).values\n",
    "            each_video_df['rolling_prob'] = prob_arr\n",
    "            \n",
    "            sort_arr = np.argsort(-prob_arr)# 全frameの中で、そのフレームのlogitsが何番目に小さいかの順番を格納したarrayを作成\n",
    "            rank_arr = np.empty_like(sort_arr) # sort_arrと同じshapeの空の配列を作成(実際は空というものはないのでランダムな値が入っている)\n",
    "            rank_arr[sort_arr] = np.arange(len(sort_arr)) # 各フレームのlogitsが全フレームのうち何番目に小さいかの順番を格納?\n",
    "            # index list for detected action\n",
    "            idx_list = []\n",
    "            for i in range(len(prob_arr)):\n",
    "                this_idx = sort_arr[i]\n",
    "                if this_idx >= 0:\n",
    "                    # Add maximam index to index_list\n",
    "                    idx_list.append(this_idx)\n",
    "                    # parityを組んで、こingnorelistを作って、順番が一定以下のものはpredictからはずす(probが高いところの周辺は最高値を残して消えていく)\n",
    "                    for parity in (-1,1):\n",
    "                        # 除外対象を考えるために、-1~1のparityに無視する範囲をかけてex_idxを作る\n",
    "                        for j in range(1, ignore_width+1):\n",
    "                            ex_idx = this_idx + j * parity\n",
    "                            # idxがprobの長さ以内にあるときに処理する\n",
    "                            if ex_idx >= 0 and ex_idx < len(prob_arr):\n",
    "                                # Exclude frames near this_idx where the action occurred. \n",
    "                                sort_arr[rank_arr[ex_idx]] = -1\n",
    "            this_df = each_video_df.iloc[idx_list].reset_index(drop=True).reset_index().rename(columns={'index':'rank'})[['rank','video_id','frame_id']]\n",
    "            this_df['event'] = event\n",
    "            train_df = train_df.append(this_df)  \n",
    "    \n",
    "    train_df['time'] = train_df['frame_id']/frame_rate\n",
    "    train_df['score'] = 1/(train_df['rank']+1)# rankに応じてスコアをつける検出個数が多いほど後ろのscoreは小さくなっていく\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>time</th>\n",
       "      <th>event</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>3036.40</td>\n",
       "      <td>challenge</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>1605.88</td>\n",
       "      <td>challenge</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>1668.44</td>\n",
       "      <td>challenge</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>2575.60</td>\n",
       "      <td>challenge</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cfbe2e94_0</td>\n",
       "      <td>332.96</td>\n",
       "      <td>challenge</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>2592.20</td>\n",
       "      <td>play</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>2511.28</td>\n",
       "      <td>play</td>\n",
       "      <td>0.032258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>2993.72</td>\n",
       "      <td>play</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>3524.40</td>\n",
       "      <td>play</td>\n",
       "      <td>0.030303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>cfbe2e94_1</td>\n",
       "      <td>3428.56</td>\n",
       "      <td>play</td>\n",
       "      <td>0.029412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id     time      event     score\n",
       "0   cfbe2e94_0  3036.40  challenge  1.000000\n",
       "1   cfbe2e94_0  1605.88  challenge  0.500000\n",
       "2   cfbe2e94_0  1668.44  challenge  0.333333\n",
       "3   cfbe2e94_0  2575.60  challenge  0.250000\n",
       "4   cfbe2e94_0   332.96  challenge  0.200000\n",
       "..         ...      ...        ...       ...\n",
       "29  cfbe2e94_1  2592.20       play  0.033333\n",
       "30  cfbe2e94_1  2511.28       play  0.032258\n",
       "31  cfbe2e94_1  2993.72       play  0.031250\n",
       "32  cfbe2e94_1  3524.40       play  0.030303\n",
       "33  cfbe2e94_1  3428.56       play  0.029412\n",
       "\n",
       "[204 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "play         70\n",
       "throwin      69\n",
       "challenge    65\n",
       "Name: event, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_df = make_sub(all_pred_logits, scoring_df)\n",
    "pp_pred_df = pp_df[[\"video_id\", \"time\", \"event\",  \"score\"]]\n",
    "display(pp_pred_df)\n",
    "pp_pred_df[\"event\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029265617113364728\n"
     ]
    }
   ],
   "source": [
    "score_after_pp = event_detection_ap(solution[solution['video_id'].isin(valid_videos)], pp_pred_df, tolerances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score just prediction:0.1882092797717849\n",
      "score after pp:0.029265617113364728\n"
     ]
    }
   ],
   "source": [
    "# つくったdataからのvalidationになるので微妙かも\n",
    "LOGGER.info(f\"score just prediction:{score_just_pred}\")\n",
    "LOGGER.info(f\"score after pp:{score_after_pp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
