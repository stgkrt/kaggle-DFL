{"cells":[{"cell_type":"markdown","metadata":{},"source":["# My baseline "]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-09-12T14:38:45.85555Z","iopub.status.busy":"2022-09-12T14:38:45.855139Z","iopub.status.idle":"2022-09-12T14:38:45.864059Z","shell.execute_reply":"2022-09-12T14:38:45.862608Z","shell.execute_reply.started":"2022-09-12T14:38:45.855504Z"},"trusted":true},"outputs":[],"source":["import glob\n","import os\n","from tqdm.auto import tqdm\n","from multiprocessing import Pool, cpu_count\n","import cv2\n","import time\n","import argparse\n","import logging\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","\n","import timm\n","from timm.models import create_model, apply_test_time_pool\n","from timm.data import ImageDataset, create_loader, resolve_data_config\n","from timm.utils import AverageMeter, setup_default_logging"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-12T14:38:45.866763Z","iopub.status.busy":"2022-09-12T14:38:45.866007Z","iopub.status.idle":"2022-09-12T14:38:45.878053Z","shell.execute_reply":"2022-09-12T14:38:45.87699Z","shell.execute_reply.started":"2022-09-12T14:38:45.866726Z"},"trusted":true},"outputs":[],"source":["if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["# Scoring function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# copy from https://www.kaggle.com/code/ryanholbrook/competition-metric-dfl-event-detection-ap\n","\n","import numpy as np\n","import pandas as pd\n","from pandas.testing import assert_index_equal\n","from typing import Dict, Tuple\n","\n","tolerances = {\n","    \"challenge\": [0.3, 0.4, 0.5, 0.6, 0.7],\n","    \"play\": [0.15, 0.20, 0.25, 0.30, 0.35],\n","    \"throwin\": [0.15, 0.20, 0.25, 0.30, 0.35],\n","}\n","\n","def filter_detections(\n","        detections: pd.DataFrame, intervals: pd.DataFrame\n",") -> pd.DataFrame:\n","    \"\"\"Drop detections not inside a scoring interval.\"\"\"\n","    detection_time = detections.loc[:, 'time'].sort_values().to_numpy()\n","    intervals = intervals.to_numpy()\n","    is_scored = np.full_like(detection_time, False, dtype=bool)\n","\n","    i, j = 0, 0\n","    while i < len(detection_time) and j < len(intervals):\n","        time = detection_time[i]\n","        int_ = intervals[j]\n","\n","        # If the detection is prior in time to the interval, go to the next detection.\n","        if time < int_.left:\n","            i += 1\n","        # If the detection is inside the interval, keep it and go to the next detection.        \n","        elif time in int_:\n","            is_scored[i] = True\n","            i += 1\n","        # If the detection is later in time, go to the next interval.\n","        else:\n","            j += 1\n","\n","    return detections.loc[is_scored].reset_index(drop=True)\n","\n","\n","def match_detections(\n","        tolerance: float, ground_truths: pd.DataFrame, detections: pd.DataFrame\n",") -> pd.DataFrame:\n","    \"\"\"Match detections to ground truth events. Arguments are taken from a common event x tolerance x video evaluation group.\"\"\"\n","    detections_sorted = detections.sort_values('score', ascending=False).dropna()\n","\n","    is_matched = np.full_like(detections_sorted['event'], False, dtype=bool)\n","    gts_matched = set()\n","    for i, det in enumerate(detections_sorted.itertuples(index=False)):\n","        best_error = tolerance\n","        best_gt = None\n","\n","        for gt in ground_truths.itertuples(index=False):\n","            error = abs(det.time - gt.time)\n","            if error < best_error and not gt in gts_matched:\n","                best_gt = gt\n","                best_error = error\n","            \n","        if best_gt is not None:\n","            is_matched[i] = True\n","            gts_matched.add(best_gt)\n","\n","    detections_sorted['matched'] = is_matched\n","\n","    return detections_sorted\n","\n","\n","def precision_recall_curve(\n","        matches: np.ndarray, scores: np.ndarray, p: int\n",") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n","    if len(matches) == 0:\n","        return [1], [0], []\n","\n","    # Sort matches by decreasing confidence\n","    idxs = np.argsort(scores, kind='stable')[::-1]\n","    scores = scores[idxs]\n","    matches = matches[idxs]\n","    \n","    distinct_value_indices = np.where(np.diff(scores))[0]\n","    threshold_idxs = np.r_[distinct_value_indices, matches.size - 1]\n","    thresholds = scores[threshold_idxs]\n","    \n","    # Matches become TPs and non-matches FPs as confidence threshold decreases\n","    tps = np.cumsum(matches)[threshold_idxs]\n","    fps = np.cumsum(~matches)[threshold_idxs]\n","    \n","    precision = tps / (tps + fps)\n","    precision[np.isnan(precision)] = 0\n","    recall = tps / p  # total number of ground truths might be different than total number of matches\n","    \n","    # Stop when full recall attained and reverse the outputs so recall is non-increasing.\n","    last_ind = tps.searchsorted(tps[-1])\n","    sl = slice(last_ind, None, -1)\n","\n","    # Final precision is 1 and final recall is 0\n","    return np.r_[precision[sl], 1], np.r_[recall[sl], 0], thresholds[sl]\n","\n","\n","def average_precision_score(matches: np.ndarray, scores: np.ndarray, p: int) -> float:\n","    precision, recall, _ = precision_recall_curve(matches, scores, p)\n","    # Compute step integral\n","    return -np.sum(np.diff(recall) * np.array(precision)[:-1])\n","\n","\n","def event_detection_ap(\n","        solution: pd.DataFrame,\n","        submission: pd.DataFrame,\n","        tolerances: Dict[str, float],\n",") -> float:\n","\n","    assert_index_equal(solution.columns, pd.Index(['video_id', 'time', 'event']))\n","    assert_index_equal(submission.columns, pd.Index(['video_id', 'time', 'event', 'score']))\n","\n","    # Ensure solution and submission are sorted properly\n","    solution = solution.sort_values(['video_id', 'time'])\n","    submission = submission.sort_values(['video_id', 'time'])\n","    \n","    # Extract scoring intervals.\n","    intervals = (\n","        solution\n","        .query(\"event in ['start', 'end']\")\n","        .assign(interval=lambda x: x.groupby(['video_id', 'event']).cumcount())\n","        .pivot(index='interval', columns=['video_id', 'event'], values='time')\n","        .stack('video_id')\n","        .swaplevel()\n","        .sort_index()\n","        .loc[:, ['start', 'end']]\n","        .apply(lambda x: pd.Interval(*x, closed='both'), axis=1)\n","    )\n","\n","    # Extract ground-truth events.\n","    ground_truths = (\n","        solution\n","        .query(\"event not in ['start', 'end']\")\n","        .reset_index(drop=True)\n","    )\n","\n","    # Map each event class to its prevalence (needed for recall calculation)\n","    class_counts = ground_truths.value_counts('event').to_dict()\n","\n","    # Create table for detections with a column indicating a match to a ground-truth event\n","    detections = submission.assign(matched = False)\n","\n","    # Remove detections outside of scoring intervals\n","    detections_filtered = []\n","    for (det_group, dets), (int_group, ints) in zip(\n","        detections.groupby('video_id'), intervals.groupby('video_id')\n","    ):\n","        assert det_group == int_group\n","        detections_filtered.append(filter_detections(dets, ints))\n","    detections_filtered = pd.concat(detections_filtered, ignore_index=True)\n","\n","    # Create table of event-class x tolerance x video_id values\n","    aggregation_keys = pd.DataFrame(\n","        [(ev, tol, vid)\n","         for ev in tolerances.keys()\n","         for tol in tolerances[ev]\n","         for vid in ground_truths['video_id'].unique()],\n","        columns=['event', 'tolerance', 'video_id'],\n","    )\n","\n","    # Create match evaluation groups: event-class x tolerance x video_id\n","    detections_grouped = (\n","        aggregation_keys\n","        .merge(detections_filtered, on=['event', 'video_id'], how='left')\n","        .groupby(['event', 'tolerance', 'video_id'])\n","    )\n","    ground_truths_grouped = (\n","        aggregation_keys\n","        .merge(ground_truths, on=['event', 'video_id'], how='left')\n","        .groupby(['event', 'tolerance', 'video_id'])\n","    )\n","    \n","    # Match detections to ground truth events by evaluation group\n","    detections_matched = []\n","    for key in aggregation_keys.itertuples(index=False):\n","        dets = detections_grouped.get_group(key)\n","        gts = ground_truths_grouped.get_group(key)\n","        detections_matched.append(\n","            match_detections(dets['tolerance'].iloc[0], gts, dets)\n","        )\n","    detections_matched = pd.concat(detections_matched)\n","    \n","    # Compute AP per event x tolerance group\n","    event_classes = ground_truths['event'].unique()\n","    ap_table = (\n","        detections_matched\n","        .query(\"event in @event_classes\")\n","        .groupby(['event', 'tolerance']).apply(\n","        lambda group: average_precision_score(\n","        group['matched'].to_numpy(),\n","                group['score'].to_numpy(),\n","                class_counts[group['event'].iat[0]],\n","            )\n","        )\n","    )\n","\n","    # Average over tolerances, then over event classes\n","    mean_ap = ap_table.groupby('event').mean().mean()\n","\n","    return mean_ap"]},{"cell_type":"markdown","metadata":{},"source":["# post process function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["event_names = ['challenge', 'throwin', 'play']\n","label_dict = {\n","    'background':0,\n","    'challenge':1,\n","    'play':2,\n","    'throwin':3,\n","}\n","event_names_with_background = ['background','challenge','play','throwin']\n","\n","def make_sub(prob, pred_df):\n","    \n","    frame_rate = 25\n","    window_size = 10\n","    ignore_width = 10\n","    group_count = 5\n","\n","    df = pd.DataFrame(prob,columns=event_names_with_background)\n","    df['video_id'] = pred_df['video_id']\n","    df['frame_id'] = pred_df['time']*frame_rate\n","\n","    train_df = pd.DataFrame()\n","    for video_id, each_video_df in df.groupby('video_id'):\n","        for i, event in enumerate(event_names):\n","            # イベント毎にwindow size分の移動平均を取る-> prob_arrに格納(最初と最後のwindow_sizeがたりない分はNanになるので-100で埋める)\n","            prob_arr = each_video_df[event].rolling(window=window_size, center=True).mean().fillna(-100).values\n","            each_video_df['rolling_prob'] = prob_arr\n","            \n","            sort_arr = np.argsort(-prob_arr)# 全frameの中で、そのフレームのlogitsが何番目に小さいかの順番を格納したarrayを作成\n","            rank_arr = np.empty_like(sort_arr) # sort_arrと同じshapeの空の配列を作成(実際は空というものはないのでランダムな値が入っている)\n","            rank_arr[sort_arr] = np.arange(len(sort_arr)) # 各フレームのlogitsが全フレームのうち何番目に小さいかの順番を格納?\n","            # index list for detected action\n","            idx_list = []\n","            for i in range(len(prob_arr)):\n","                this_idx = sort_arr[i]\n","                if this_idx >= 0:\n","                    # Add maximam index to index_list\n","                    idx_list.append(this_idx)\n","                    # parityを組んで、こingnorelistを作って、順番が一定以下のものはpredictからはずす(probが高いところの周辺は最高値を残して消えていく)\n","                    for parity in (-1,1):\n","                        # 除外対象を考えるために、-1~1のparityに無視する範囲をかけてex_idxを作る\n","                        for j in range(1, ignore_width+1):\n","                            ex_idx = this_idx + j * parity\n","                            # idxがprobの長さ以内にあるときに処理する\n","                            if ex_idx >= 0 and ex_idx < len(prob_arr):\n","                                # Exclude frames near this_idx where the action occurred. \n","                                sort_arr[rank_arr[ex_idx]] = -1\n","            this_df = each_video_df.iloc[idx_list].reset_index(drop=True).reset_index().rename(columns={'index':'rank'})[['rank','video_id','frame_id']]\n","            this_df['event'] = event\n","            train_df = train_df.append(this_df)  \n","    \n","    train_df['time'] = train_df['frame_id']/frame_rate\n","    train_df['score'] = 1/(train_df['rank']+1)# rankに応じてスコアをつける検出個数が多いほど後ろのscoreは小さくなっていく\n","    \n","    return train_df"]},{"cell_type":"markdown","metadata":{},"source":["# Configurations"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-12T14:38:45.882138Z","iopub.status.busy":"2022-09-12T14:38:45.880485Z","iopub.status.idle":"2022-09-12T14:38:45.887792Z","shell.execute_reply":"2022-09-12T14:38:45.886945Z","shell.execute_reply.started":"2022-09-12T14:38:45.882108Z"},"trusted":true},"outputs":[],"source":["DEBUG = False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-12T14:38:45.890145Z","iopub.status.busy":"2022-09-12T14:38:45.889216Z","iopub.status.idle":"2022-09-12T14:38:45.898062Z","shell.execute_reply":"2022-09-12T14:38:45.897061Z","shell.execute_reply.started":"2022-09-12T14:38:45.890052Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    EXP = \"eff_b5_ap_bce_flowimage\"\n","    # model\n","    model_type =  \"tf_efficientnet_b5_ap\"\n","    # trained_model_path = \"/workdir/work/output/eff_b5_ap/tf_efficientnet_b5_ap.pth\"\n","    trained_model_path = \"/workdir/work/output/eff_b5_ap_bce_flowimage/effinet_b5_ap__flowimage_224.pth\"\n","    out_features = 4 # output class\n","    inp_channels = 3 #RGB -> 3\n","    dropout = 0\n","    pretrained = False\n","    \n","    IMG_SIZE = (224, 224)\n","\n","\n","    # optical flow settings\n","    # params for ShiTomasi corner detection\n","    feature_params = dict( maxCorners = 100,\n","                        qualityLevel = 0.3,\n","                        minDistance = 7,\n","                        blockSize = 7 )\n","\n","    # Parameters for lucas kanade optical flow\n","    lk_params = dict( winSize  = (15,15),\n","                    maxLevel = 2,\n","                    criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n","    \n","    crop_range = 600\n","    area_thr = 0.1\n","    FLOW_CAL_TIME = 0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-12T14:38:45.900034Z","iopub.status.busy":"2022-09-12T14:38:45.899537Z","iopub.status.idle":"2022-09-12T14:38:45.907671Z","shell.execute_reply":"2022-09-12T14:38:45.906748Z","shell.execute_reply.started":"2022-09-12T14:38:45.899995Z"},"trusted":true},"outputs":[],"source":["event_decoding = {\n","    0 : \"background\",\n","    1 : \"challenge\",\n","    2 : \"play\",\n","    3 : \"throwin\",\n","}"]},{"cell_type":"markdown","metadata":{},"source":["# optical flow functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_maxflow_area(x, y, width, height, crop_range=CFG.crop_range):\n","    crop_x_min = int(x) - (crop_range/2)\n","    crop_x_max = int(x) + (crop_range/2)\n","    crop_x_min = max(0, crop_x_min)\n","    crop_x_max = min(width, crop_x_max)\n","\n","    crop_y_min = int(y) - (crop_range/2)\n","    crop_y_max = int(y) + (crop_range/2)\n","    crop_y_min = max(0, crop_y_min)\n","    crop_y_max = min(height, crop_y_max)\n","\n","    return int(crop_x_min), int(crop_x_max), int(crop_y_min), int(crop_y_max)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_green_area(img):\n","    # HSV\n","    img_HSV = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n","\n","    # creat green mask (H value for green is from 100/360*179 until 180/360*179) in OpenCV\n","    lower_green = np.array([100 / 360 * 179, 0, 0])\n","    upper_green = np.array([180 / 360 * 179, 255, 255])\n","    green_mask = cv2.inRange(img_HSV, lower_green, upper_green)\n","\n","    # crop green area\n","    img_green_masked = cv2.bitwise_and(img, img, mask=green_mask)\n","    img_green_masked = cv2.cvtColor(img_green_masked, cv2.COLOR_BGR2GRAY)\n","\n","    # Find contours\n","    contours, hierarchy = cv2.findContours(img_green_masked, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n","    if not contours:\n","        return int(img.shape[1]*0.5), int(img.shape[0]*0.5), img.shape[1], img.shape[0]\n","    # Find the contour with the maximum area.\n","    c = max(contours, key=cv2.contourArea)\n","\n","    # Get bounding rectangle\n","    x, y, w, h = cv2.boundingRect(c)\n","\n","    return x, y, w, h"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def crop_maxflow_area(cap_, base_frame_, fps_, frame_num_):\n","    \"\"\"色相が緑っぽいエリアをフィールドとしてクロップして、optical flowを計算。1secでflowが最も大きい点を中心としてcropする。\n","    \n","    \"\"\"\n","    # optical flow line color\n","    line_color = [255, 100, 0]\n","    # base frameでフィールドの範囲を決める。1secで大きくカメラ方向が移動するときはだめかも\n","    x_field, y_field, w_field, h_field = get_green_area(base_frame_)\n","    base_frame_ = base_frame_[y_field:y_field+h_field, x_field:x_field+w_field, :]\n","\n","    # Take first frame and find corners in it\n","    base_gray = cv2.cvtColor(base_frame_, cv2.COLOR_BGR2GRAY)\n","    base_point = cv2.goodFeaturesToTrack(base_gray, mask = None, **CFG.feature_params)\n","    # Create a mask image for drawing purposes\n","    mask = np.zeros_like(base_frame_)\n","\n","    # draw the tracks\n","    max_distance = 0\n","    img_max_dist = base_frame_\n","\n","    # 1secで基準画像とのoptical flowが最も大きい点を残す\n","    for i in range(int(fps_*CFG.FLOW_CAL_TIME)):\n","        successed, relative_frame = cap_.read()\n","        if not successed:\n","            break\n","        frame_num_ += 1\n","\n","        relative_frame = relative_frame[y_field : y_field+h_field, x_field : x_field+w_field, :]\n","        relative_gray = cv2.cvtColor(relative_frame, cv2.COLOR_BGR2GRAY)\n","\n","        # calculate optical flow\n","        relative_point, st, err = cv2.calcOpticalFlowPyrLK(base_gray, relative_gray, base_point, None, **CFG.lk_params)\n","\n","        # Select good points\n","        good_new = relative_point[st==1]\n","        good_old = base_point[st==1]\n","\n","        x_max_distance = int(x_field + (w_field*0.5))\n","        y_max_distance = int(y_field + (h_field*0.5))\n","        distance_sum = 0\n","        for i, (new, old) in enumerate(zip(good_new, good_old)):\n","            x_after, y_after = new.ravel()\n","            x_before, y_before = old.ravel()\n","            distance = np.sqrt( (x_after - x_before)**2 + (y_after - y_before)**2 )\n","            distance_sum += distance\n","            # distanceが大きい、かつ、検出点が画像の外側area_thr%にないときに移動距離が最大と判定する\n","            if distance > max_distance and (w_field*CFG.area_thr < x_after < w_field*(1.0 -CFG.area_thr)) and (h_field*CFG.area_thr < y_after < y_field*(1.0 - CFG.area_thr)):\n","                max_distance = distance\n","                x_max_distance = x_after\n","                y_max_distance = y_after\n","                # 画像にoptical flow の線を追加する(本当はtop5だけ描くとかの方がいいかも？)\n","                mask = cv2.line(mask, (int(x_after),int(y_after)), (int(x_before), int(y_before)), line_color, 2)\n","                relative_frame = cv2.line(relative_frame, (int(x_after),int(y_after)), (int(x_before), int(y_before)), line_color, 2)\n","                img_max_dist = cv2.circle(relative_frame, (int(x_after),int(y_after)), 5, line_color, -1)\n","    distance_mean = distance_sum/(i+1)\n","    # optical flowの最も大きい点を中心にクロップ(サイズが足りないときは横方向だけとか縦方向だけの場合もあり)\n","    x_min, x_max, y_min, y_max = get_maxflow_area(int(x_max_distance), int(y_max_distance), w_field, h_field)\n","    croped_flow_image = img_max_dist[y_min:y_max, x_min:x_max, :]\n","\n","    return croped_flow_image, distance_mean"]},{"cell_type":"markdown","metadata":{},"source":["# set inference files"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-12T14:38:45.910165Z","iopub.status.busy":"2022-09-12T14:38:45.909061Z","iopub.status.idle":"2022-09-12T14:38:45.91955Z","shell.execute_reply":"2022-09-12T14:38:45.918277Z","shell.execute_reply.started":"2022-09-12T14:38:45.910132Z"},"trusted":true},"outputs":[],"source":["valid_video_files = ['/workdir/work/input/train/cfbe2e94_0.mp4', '/workdir/work/input/train/cfbe2e94_1.mp4']\n","print(valid_video_files)"]},{"cell_type":"markdown","metadata":{},"source":["# Load model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-12T14:38:45.921493Z","iopub.status.busy":"2022-09-12T14:38:45.92099Z","iopub.status.idle":"2022-09-12T14:38:45.929873Z","shell.execute_reply":"2022-09-12T14:38:45.928777Z","shell.execute_reply.started":"2022-09-12T14:38:45.921461Z"},"trusted":true},"outputs":[],"source":["class DFLNet(nn.Module):\n","    def __init__(self, model_name=CFG.model_type, \n","                 out_features=CFG.out_features, inp_channels=CFG.inp_channels,\n","                 pretrained=CFG.pretrained):\n","        super().__init__()\n","        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels, num_classes=out_features)\n","    \n","    def forward(self, image):\n","        output = self.model(image)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-12T14:38:45.932538Z","iopub.status.busy":"2022-09-12T14:38:45.931729Z","iopub.status.idle":"2022-09-12T14:38:47.052774Z","shell.execute_reply":"2022-09-12T14:38:47.051655Z","shell.execute_reply.started":"2022-09-12T14:38:45.932499Z"},"trusted":true},"outputs":[],"source":["model = DFLNet()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-12T14:38:47.057595Z","iopub.status.busy":"2022-09-12T14:38:47.057252Z","iopub.status.idle":"2022-09-12T14:38:47.872612Z","shell.execute_reply":"2022-09-12T14:38:47.87151Z","shell.execute_reply.started":"2022-09-12T14:38:47.057565Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["model.load_state_dict(torch.load(CFG.trained_model_path))\n","model.to(device)\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-12T14:38:47.874643Z","iopub.status.busy":"2022-09-12T14:38:47.87415Z","iopub.status.idle":"2022-09-12T14:38:47.882643Z","shell.execute_reply":"2022-09-12T14:38:47.881375Z","shell.execute_reply.started":"2022-09-12T14:38:47.874602Z"},"trusted":true},"outputs":[],"source":["def image_read_formodel(image):\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    image = cv2.resize(image, dsize=CFG.IMG_SIZE)\n","    image = image / 255 # convert to 0-1\n","    image = image.reshape(-1, CFG.inp_channels, CFG.IMG_SIZE[0], CFG.IMG_SIZE[1])\n","    return torch.tensor(image, dtype=torch.float)"]},{"cell_type":"markdown","metadata":{},"source":["# Validation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submission_list = []\n","softmax = nn.Softmax(dim=1)\n","all_pred_logits = []\n","distance_mean_list = []\n","\n","for video_path in valid_video_files:\n","    video_name = os.path.basename(video_path).split('.')[0]\n","    cam = cv2.VideoCapture(video_path)\n","    fps = cam.get(cv2.CAP_PROP_FPS)\n","    video_file = video_path.split(\"/\")[-1]\n","    video_id = video_file.split(\".\")[0]\n","    print(\"video_id:\", video_id)\n","    frame_count = 1\n","    preds_prob_video = []\n","    pred_subformat_list = []\n","    start_time = time.time()\n","    while True:\n","        successed, base_frame = cam.read()\n","        if not successed:\n","            break\n","        img, distance_mean = crop_maxflow_area(cam, base_frame, fps, frame_count)\n","        distance_mean_list.append(distance_mean)\n","        if distance_mean < 4:\n","            continue\n","        img = image_read_formodel(img)\n","        img = img.to(device)\n","        output = model(img)\n","        output = softmax(output)\n","        output = output.to('cpu').detach().numpy().copy()\n","        pred_logits = output[0]\n","        all_pred_logits.append(pred_logits)\n","        # make pred for submission        \n","        pred_argmax_idx = np.argmax(pred_logits, axis=0)\n","        pred_prob = pred_logits[pred_argmax_idx]\n","        pred_event = event_decoding[pred_argmax_idx]\n","        if pred_argmax_idx != 0:\n","            frame_count = cam.get(cv2.CAP_PROP_POS_FRAMES)\n","            current_time = frame_count/fps\n","            pred_subformat_list.append([video_id, current_time, pred_event, pred_prob])\n","        else:\n","            frame_count = cam.get(cv2.CAP_PROP_POS_FRAMES)\n","            current_time = frame_count/fps\n","            pred_subformat_list.append([video_id, current_time, \"play\", 0.33])\n","        if DEBUG and (frame_count > 100):\n","            break\n","    if len(submission_list) == 0:\n","        submission_list = pred_subformat_list\n","    else:\n","        submission_list.extend(pred_subformat_list)\n","    \n","    elapsed_time = time.time() - start_time\n","    print(f\"Elapsed time:{elapsed_time}, average:{elapsed_time/60} min.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(all_pred_logits)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-12T14:41:04.799683Z","iopub.status.busy":"2022-09-12T14:41:04.799303Z","iopub.status.idle":"2022-09-12T14:41:04.830946Z","shell.execute_reply":"2022-09-12T14:41:04.830021Z","shell.execute_reply.started":"2022-09-12T14:41:04.799646Z"},"trusted":true},"outputs":[],"source":["valid_df = pd.DataFrame(submission_list, columns=[\"video_id\", \"time\", \"event\", \"score\"])\n","display(valid_df.head(5))\n","valid_df[\"event\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["valid_df[\"score\"].hist(bins=50)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn import preprocessing\n","mm = preprocessing.MinMaxScaler()\n","valid_scale_df = valid_df\n","valid_scale_df[\"score\"] = mm.fit_transform(valid_df[\"score\"].values.reshape(-1,1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["valid_scale_df[\"score\"].hist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["valid_scale_not_df = valid_scale_df[valid_scale_df[\"score\"] != 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-12T14:41:04.832931Z","iopub.status.busy":"2022-09-12T14:41:04.832358Z","iopub.status.idle":"2022-09-12T14:41:04.845309Z","shell.execute_reply":"2022-09-12T14:41:04.844235Z","shell.execute_reply.started":"2022-09-12T14:41:04.832897Z"},"trusted":true},"outputs":[],"source":["valid_df.to_csv(f\"/workdir/work/output/{CFG.EXP}/cap_flowimage_valid_distancemean.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["valid_df[\"event\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["valid_scale_not_df[\"event\"].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["# validation scoring"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-12T14:41:04.88026Z","iopub.status.busy":"2022-09-12T14:41:04.879548Z","iopub.status.idle":"2022-09-12T14:41:04.903579Z","shell.execute_reply":"2022-09-12T14:41:04.902274Z","shell.execute_reply.started":"2022-09-12T14:41:04.880227Z"},"trusted":true},"outputs":[],"source":["solution = pd.read_csv(\"/workdir/work/input/train.csv\", usecols=['video_id', 'time', 'event'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-12T14:41:04.905352Z","iopub.status.busy":"2022-09-12T14:41:04.90501Z","iopub.status.idle":"2022-09-12T14:41:04.911918Z","shell.execute_reply":"2022-09-12T14:41:04.910767Z","shell.execute_reply.started":"2022-09-12T14:41:04.90531Z"},"trusted":true},"outputs":[],"source":["valid_video_files"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-12T14:41:04.913958Z","iopub.status.busy":"2022-09-12T14:41:04.913571Z","iopub.status.idle":"2022-09-12T14:41:04.920887Z","shell.execute_reply":"2022-09-12T14:41:04.919756Z","shell.execute_reply.started":"2022-09-12T14:41:04.913924Z"},"trusted":true},"outputs":[],"source":["valid_video_id = [valid_id.split(\"/\")[-1].split(\".\")[0] for valid_id in valid_video_files]\n","print(valid_video_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-12T14:41:04.922958Z","iopub.status.busy":"2022-09-12T14:41:04.92246Z","iopub.status.idle":"2022-09-12T14:41:04.942078Z","shell.execute_reply":"2022-09-12T14:41:04.941064Z","shell.execute_reply.started":"2022-09-12T14:41:04.922926Z"},"trusted":true},"outputs":[],"source":["solution[solution['video_id'].isin(valid_video_id)]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-12T14:43:57.71263Z","iopub.status.busy":"2022-09-12T14:43:57.712263Z","iopub.status.idle":"2022-09-12T14:43:57.899475Z","shell.execute_reply":"2022-09-12T14:43:57.898293Z","shell.execute_reply.started":"2022-09-12T14:43:57.712599Z"},"trusted":true},"outputs":[],"source":["score_before_pp = event_detection_ap(solution[solution['video_id'].isin(valid_video_id)], valid_df, tolerances)\n","score_scaled = event_detection_ap(solution[solution['video_id'].isin(valid_video_id)], valid_scale_not_df, tolerances)"]},{"cell_type":"markdown","metadata":{},"source":["# Post Processing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pp_df = make_sub(all_pred_logits, valid_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pp_pred_df = pp_df[[\"video_id\", \"time\", \"event\",  \"score\"]]\n","display(pp_pred_df)\n","pp_pred_df[\"event\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["score_after_pp = event_detection_ap(solution[solution['video_id'].isin(valid_video_id)], pp_pred_df, tolerances)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(score_before_pp)\n","print(score_scaled)\n","print(score_after_pp)"]},{"cell_type":"markdown","metadata":{},"source":["# Score(memo)"]},{"cell_type":"markdown","metadata":{},"source":["## before pp"]},{"cell_type":"markdown","metadata":{},"source":["- focalloss : 0.011592382625658445\n","- crossentropy : 0.005342858648023062\n","- bce(flow image): 0.0147564709424373\n","\n","#### with crop\n","- bce (flow image, 3sec): 0.013892144681695594\n","- bce (flow image, 1sec): 0.023892144681695594\n","- bce (flow image, 1sec, probchage): 0.020262194957537494\n","- bce (flow image, 0.5 sec, probchage): 0.039299473910823224"]},{"cell_type":"markdown","metadata":{},"source":["## after pp"]},{"cell_type":"markdown","metadata":{},"source":["- bce (flow image) : 0.06103998460334889\n","\n","#### with crop\n","- bce (flow image, 3sec) :  0.0019885161915018453\n","- bce (flow image, 3sec) :  0.01214557056162458\n","- bce (flow image, 1sec, probchage) : 0.005615242633485843\n","- bce (flow image, 0.5sec, probchage) : 0.006466410476529082"]},{"cell_type":"markdown","metadata":{},"source":["# checking "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["frame_rate = 25\n","window_size = 10\n","ignore_width = 10\n","group_count = 5\n","\n","df = pd.DataFrame(all_pred_logits,columns=event_names_with_background)\n","df['video_id'] = valid_df['video_id']\n","df['frame_id'] = valid_df['time']*frame_rate\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for video_id, each_video_df in df.groupby('video_id'):\n","    for i, event in enumerate(event_names):\n","        print(event)\n","        # each_video_df[event].rolling(window=window_size, center=True).mean().fillna(-100).values\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["each_video_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rolling_df = each_video_df[\"challenge\"].rolling(window=window_size, center=True).mean()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rolling_df.head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["prob_arr = each_video_df[event].rolling(window=window_size, center=True).mean().fillna(-100).values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["prob_arr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.argsort(-prob_arr)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sort_arr = np.argsort(-prob_arr)\n","rank_arr = np.empty_like(sort_arr)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rank_arr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rank_arr[sort_arr] = np.arange(len(sort_arr))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rank_arr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(len(prob_arr)):\n","    this_idx = sort_arr[i]\n","    print(this_idx)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
